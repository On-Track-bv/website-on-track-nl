# Crawler Bescherming voor On-Track Website

Deze website heeft **gemiddelde** crawler bescherming geÃ¯mplementeerd - effectief tegen agressieve scrapers maar vriendelijk voor legitieme zoekmachines en gebruikers.

## Huidige Configuratie: **MODERATE LEVEL**

### âœ… **Wat wordt toegestaan:**
- Google, Bing, Yahoo zoekmachines
- Normale gebruikers met alle browsers
- Developer tools en right-click
- Indexering door zoekmachines

### âŒ **Wat wordt geblokkeerd:**
- Agressieve scrapers (SemrushBot, AhrefsBot, etc.)
- Python-requests, wget, curl scripts
- Headless browsers zonder legitieme reden
- Excessieve hotlinking

## GeÃ¯mplementeerde Beschermingslagen

### 1. **robots.txt** (`/public/robots.txt`)
- âœ… Staat legitieme zoekmachines toe (Google, Bing, Yahoo)
- âŒ Blokkeert agressieve scrapers (Semrush, Ahrefs, etc.)
- â±ï¸ Crawl-delay van 1 seconde voor respectvolle crawling
- ğŸ”’ Beschermt /admin/, /api/, /private/ folders

### 2. **HTML Meta Tags** (`/index.html`)
- âœ… `index, follow` - Zoekmachines mogen indexeren
- âŒ `noarchive` - Voorkomt caching van pagina's
- âŒ `nosnippet` - Voorkomt snippets in zoekresultaten

### 3. **React Crawler Protection Component** (Moderate Level)
Detecteert alleen:
- âŒ **Agressieve scrapers**: Python-requests, wget, curl, etc.
- âŒ **Commercial bots**: SemrushBot, AhrefsBot, MJ12bot
- âœ… **Toegestaan**: Normale browsers, developer tools, right-click

### 4. **Server-level bescherming** (`.htaccess`)
- âŒ Blokkeert alleen agressieve scraper user-agents
- âœ… Toegang voor Google, Bing referrers bij afbeeldingen
- ğŸ”’ Beschermt gevoelige bestanden (.env, .log)
- ğŸ›¡ï¸ Basis security headers

### 5. **Development/Preview Headers** (`vite.config.ts`)
- Matige X-Robots-Tag headers (noarchive, nosnippet)
- SAMEORIGIN frame protection (niet DENY)

## Beschermingsniveaus

### ğŸŸ¢ **Mild** (mild level)
```tsx
<CrawlerProtection level="mild">
```
- Alleen robots.txt filtering
- Geen JavaScript detectie
- Geen keyboard/mouse blokkering

### ğŸŸ¡ **Moderate** (huidige configuratie)
```tsx
<CrawlerProtection level="moderate">
```
- âœ… Agressieve scrapers geblokkeerd
- âœ… Zoekmachines toegestaan
- âœ… Developer tools beschikbaar
- âœ… Right-click werkt normaal

### ğŸ”´ **Strict** (maximale bescherming)
```tsx
<CrawlerProtection level="strict">
```
- Alle bots geblokkeerd
- Headless browser detectie
- F12/Ctrl+U geblokkeerd
- Right-click uitgeschakeld

## Voordelen van Moderate Level

âœ… **SEO-vriendelijk**: Google en Bing kunnen je site indexeren  
âœ… **Gebruikersvriendelijk**: Normale browserfunctionaliteit behouden  
âœ… **Developer-vriendelijk**: F12 en developer tools werken  
âœ… **Effectieve bescherming**: Agressieve scrapers worden gestopt  
âœ… **Evenwichtig**: Goede balans tussen bescherming en toegankelijkheid  

## Testen van je bescherming

```bash
# Test legitieme crawler (zou moeten werken)
curl -H "User-Agent: Googlebot/2.1" https://jouw-website.com

# Test agressieve scraper (zou geblokkeerd moeten worden)
curl -H "User-Agent: SemrushBot/7~bl" https://jouw-website.com

# Check robots.txt
curl https://jouw-website.com/robots.txt

# Check headers
curl -I https://jouw-website.com
```

## Aanpassingen maken

### Voor meer bescherming â†’ Strict:
```tsx
<CrawlerProtection level="strict" />
```

### Voor minder bescherming â†’ Mild:
```tsx
<CrawlerProtection level="mild" />
```

### Custom detectie handler:
```tsx
<CrawlerProtection 
  level="moderate"
  onCrawlerDetected={() => {
    // Log naar analytics
    gtag('event', 'crawler_blocked');
    console.log('Scraper gedetecteerd');
  }}
/>
```
